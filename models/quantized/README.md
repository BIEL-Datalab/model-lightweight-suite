# 量化模型目录

## 目录说明

本目录包含经过量化的模型，根据量化精度分为FP16和INT8两个子目录。

## 目录结构

```
quantized/
├── fp16/                        # FP16半精度量化模型
├── int8/                        # INT8低精度量化模型
└── README.md                    # 目录说明文档
```

## 子目录说明

### fp16/
包含FP16半精度量化模型，FP16量化可以在保持较高精度的同时减少模型大小和加速推理。

### int8/
包含INT8低精度量化模型，INT8量化可以大幅减少模型大小和加速推理，但可能会带来一定的精度损失。

## 文件命名规范

量化模型文件应遵循以下命名规范：

```
{模型名称}_{数据集名称}_{量化工具}_{精度}_{设备}.{文件格式}
```

示例：
- `resnet50_imagenette_modelopt_int8_x86.engine`
- `resnet50_imagenette_torch_int8.pth`
- `resnet50_imagenette_trt_int8_jetson.engine`

## 使用说明

1. **选择量化精度**：根据需求选择合适的量化精度，FP16精度较高，INT8速度更快
2. **选择模型文件**：根据部署设备选择相应的模型文件，如x86设备选择带有x86后缀的文件，Jetson设备选择带有jetson后缀的文件
3. **加载模型**：根据模型格式使用相应的加载方法，如：
   - PyTorch模型：`torch.load("model_path.pth")`
   - ONNX模型：`onnx.load("model_path.onnx")`
   - TensorRT引擎：使用TensorRT API加载

## 量化工具说明

当前支持的量化工具包括：

- **ModelOpt**：开源模型优化工具，支持多种后端
- **TensorRT**：NVIDIA TensorRT，用于GPU加速推理
- **Torch**：PyTorch内置的量化功能

## 注意事项

1. **精度与速度权衡**：FP16精度较高但速度提升有限，INT8速度快但精度可能降低
2. **设备兼容性**：不同设备可能需要不同格式的模型文件
3. **工具链选择**：根据部署环境选择合适的量化工具链
4. **性能测试**：在实际部署设备上测试量化模型的性能和精度
5. **模型验证**：量化前后应对模型进行验证，确保精度损失在可接受范围内
6. **文件管理**：定期清理过期的量化模型文件，避免占用过多存储空间
